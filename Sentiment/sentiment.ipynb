{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Regular expression operations\n",
    "import re\n",
    "\n",
    "# Natural Language Toolkit\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Deep learning libraries\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Utility libraries\n",
    "import random\n",
    "\n",
    "# Balance data sets\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Evolutionary computation\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "# Hypothesis testing\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get posts and articles from ProfitsBot \n",
    "csv_path = '/ProfitsBot_V0_OLLM/ds_builder/combined_data.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Get USE embeddings\n",
    "use_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"\n",
    "use_model = hub.load(use_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average score of token stream\n",
    "def average_glove_vectors(tokens):\n",
    "    vectors = [glove_embeddings[token] for token in tokens if token in glove_embeddings]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(len(next(iter(glove_embeddings.values()))))\n",
    "\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    # Standardise words via lemmantisation\n",
    "    def get_wordnet_pos(word):\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "    \n",
    "    lemmas = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n",
    "    \n",
    "    return lemmas\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Get rid of symbols and links ect\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d', '', text)\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = lemmatize_tokens(tokens)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove_path = '/emebeddings/glove.6B.50d.txt'\n",
    "glove_embeddings = {}\n",
    "with open(glove_path, encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        glove_embeddings[word] = vector\n",
    "\n",
    "\n",
    "# Apply preprocessing to title in data frame\n",
    "df['title'] = df['title'].apply(preprocess_text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF initialisation\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['title'].apply(lambda x: ' '.join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens into lists = corpus\n",
    "corpus = df['title'].apply(lambda x: ' '.join(x)).tolist()\n",
    "\n",
    "# Function to convert a single text into its GloVe embedding\n",
    "def text_to_glove_embedding(text):\n",
    "    embedding = np.zeros(50)  \n",
    "    for word in text.split():\n",
    "        if word in glove_embeddings:\n",
    "            embedding += glove_embeddings[word]\n",
    "    return embedding\n",
    "\n",
    "X_glove = np.array([text_to_glove_embedding(text) for text in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to obtain USE embeddings\n",
    "def text_to_use_embedding(text):\n",
    "    embedding = use_model([text])[0].numpy()\n",
    "    return embedding\n",
    "\n",
    "X_utf = np.array([text_to_use_embedding(text) for text in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set min fitness (just base fitness) / define individual - list eg - [0,1,1,1,0]\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "creator.create(\"Individual\", list, typecode=\"d\", fitness=creator.FitnessMin, strategy=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifers\n",
    "classifiers = {\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=3),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "    'SVM': SVC(kernel='linear', C=1.0, random_state=42,probability=True),\n",
    "}\n",
    "\n",
    "# Apply even class distribution via SMOTE\n",
    "def apply_smote(X, y):\n",
    "    smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "\n",
    "# Parameter grid defined \n",
    "alpha_grid = [0.47,0.5,0.55]\n",
    "embeddings = ['TF-IDF','GloVe','UTF']\n",
    "\n",
    "# Set to 0 for initial attempt\n",
    "multi_day = 1 \n",
    "\n",
    "def eval(individual):\n",
    "\n",
    "    # Where we store all the probabilities for each day \n",
    "    day_hashmap = {}\n",
    "\n",
    "    param_grid = {\n",
    "    'KNN': {\n",
    "        'n_neighbors': [3, 5, 7],\n",
    "        'weights': ['uniform', 'distance','uniform']\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf', 'poly']\n",
    "    },\n",
    "    }\n",
    "    \n",
    "    # We do MOD 3 in case mutation causes value outside of range\n",
    "    embedding_method = embeddings[int(individual[1])%3]\n",
    "\n",
    "    alpha = alpha_grid[int(individual[-1])%3]\n",
    "\n",
    "    split_index = int(len(df) * 0.8) \n",
    "\n",
    "    # Apply different embeddings depending on gene encoding\n",
    "    if embedding_method == 'TF-IDF':\n",
    "        X_embedding = X_tfidf\n",
    "    elif embedding_method == 'GloVe':\n",
    "        X_embedding = np.array([text_to_glove_embedding(text) for text in corpus])\n",
    "    elif embedding_method == 'UTF':\n",
    "        X_embedding = np.array([text_to_use_embedding(text) for text in corpus])\n",
    "\n",
    "\n",
    "    # Split days \n",
    "    X_train = X_embedding[:split_index]\n",
    "    y_train = df['sell'][:split_index]\n",
    "    X_test = X_embedding[split_index:]\n",
    "    y_test = df['sell'][split_index:]\n",
    "    test_dates = df['Date'][split_index:].to_numpy() \n",
    "    multi_dates = sorted(list(set(test_dates)))\n",
    "    X_train, y_train = apply_smote(X_train, y_train)  \n",
    "\n",
    "    classifier = list(classifiers.keys())[int(individual[0])%3]\n",
    "    params = param_grid[classifier]\n",
    "\n",
    "    threshold = alpha\n",
    "\n",
    "    # Apply params based of classifier\n",
    "    if classifier == 'KNN':\n",
    "        params['n_neighbors'] = params['n_neighbors'][int(individual[2])%3]\n",
    "        params['weights'] = params['weights'][int(individual[3])%3]\n",
    "    elif classifier == 'Random Forest':\n",
    "        params['n_estimators'] = params['n_estimators'][int(individual[2])%3]\n",
    "        params['max_depth'] = params['max_depth'][int(individual[3])%3]\n",
    "    elif classifier == 'SVM':\n",
    "        params['C'] = params['C'][int(individual[2])%3]\n",
    "        params['kernel'] = params['kernel'][int(individual[3])%3]\n",
    "    \n",
    "\n",
    "    model = classifiers[classifier].set_params(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    if multi_day:\n",
    "        # Predict probabilities for posts in test set \n",
    "        probs = model.predict_proba(X_test)\n",
    "\n",
    "        \n",
    "        # Get a hashmap of date - probability pairs \n",
    "        for i in range(len(probs)):\n",
    "            if test_dates[i] in day_hashmap:\n",
    "                day_hashmap[test_dates[i]].append(probs[i][0])\n",
    "            else:\n",
    "                day_hashmap[test_dates[i]] = [probs[i][0]]\n",
    "\n",
    "        # Binary classifier - predicted for each post\n",
    "        predicted = []\n",
    "        \n",
    "        for i in range(0,len(multi_dates)):\n",
    "            # This is 3 days considered but 2 / 1 days are same intuition \n",
    "            today = day_hashmap[multi_dates[i]]\n",
    "            yday = day_hashmap[multi_dates[max(0,i-1)]]\n",
    "            yday2 = day_hashmap[multi_dates[max(0,i-2)]]\n",
    "            multidate_probs = today + yday + yday2\n",
    "            dateProbs = len(today)\n",
    "            multilen = len(multidate_probs)\n",
    "\n",
    "            # Ensemble \n",
    "            average = (sum(multidate_probs) / multilen)\n",
    "            if average >= threshold:\n",
    "                predicted.extend([0]*dateProbs)\n",
    "            else:\n",
    "                predicted.extend([1]*dateProbs)\n",
    "    else:\n",
    "        # Initial attempt\n",
    "        predicted = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(np.array(predicted), y_test)  \n",
    "\n",
    "    # Fitness = accuracy = sum(predicted - actual)\n",
    "    fitness_values = (accuracy,)\n",
    "    return fitness_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.25384912533967\n",
      "7.447645733460462e-08\n",
      "Test Statistic (z): 5.25384912533967\n",
      "P-value: 7.447645733460462e-08\n",
      "Reject null hypothesis: The first algorithm is statistically significantly better.\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of best model vs linear \n",
    "accuracy_1 = 0.538\n",
    "accuracy_2 = 0.5\n",
    "\n",
    "# Size of the test set \n",
    "n_1 = n_2 = 9544\n",
    "\n",
    "# Pooled sample proportion\n",
    "p_hat = (accuracy_1 * n_1 + accuracy_2 * n_2) / (n_1 + n_2)\n",
    "\n",
    "# Test statistic\n",
    "z = (accuracy_1 - accuracy_2) / np.sqrt(p_hat * (1 - p_hat) * (1/n_1 + 1/n_2))\n",
    "\n",
    "p_value = 1 - norm.cdf(z)\n",
    "\n",
    "# Print results\n",
    "print(\"Test Statistic (z):\", z)\n",
    "print(\"P-value:\", p_value)\n",
    "\n",
    "# Significance level\n",
    "alpha = 0.05\n",
    "\n",
    "# Make decision\n",
    "if p_value < alpha:\n",
    "    print(\"Reject null hypothesis: The first algorithm is statistically significantly better.\")\n",
    "else:\n",
    "    print(\"Fail to reject null hypothesis: There is not enough evidence to suggest a difference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monthly_returns():\n",
    "\n",
    "    day_hashmap = {}\n",
    "\n",
    "    # Just for 2022 - as in test set \n",
    "    start_date = '2022-01-01'  \n",
    "    end_date = '2022-12-31'   \n",
    "\n",
    "    # Filter out any that are not in date range (2022)\n",
    "    filtered_df = df[(df['Date'] >= start_date) & (df['Date'] <= end_date)]\n",
    "    unique_dates_df = filtered_df.drop_duplicates(subset=['Date'])\n",
    "\n",
    "    # Filter the hashmap\n",
    "    filtered_hashmap = {date: value for date, value in day_hashmap.items() if date.startswith('2022')}\n",
    "    date_sell_hashmap = {pd.to_datetime(key): value for key, value in filtered_hashmap.items()}\n",
    "    unique_dates_df['Date'] = pd.to_datetime(unique_dates_df['Date'])\n",
    "\n",
    "    # Check accuracy\n",
    "    def is_sell_correct(row):\n",
    "        date = row['Date']\n",
    "        sell_value = row['sell']\n",
    "        return date in date_sell_hashmap and date_sell_hashmap[date] == sell_value\n",
    "\n",
    "    # Ignore replicated dates as all predictions in a day will be the same \n",
    "    unique_dates_df['is_sell_correct'] = unique_dates_df.apply(is_sell_correct, axis=1)\n",
    "    print(unique_dates_df)\n",
    "    monthly_correct_percentages = unique_dates_df.groupby(unique_dates_df['Date'].dt.to_period('M'))['is_sell_correct'].mean() * 100\n",
    "    print(monthly_correct_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_monthly_returns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profitability():\n",
    "    csv_path = '../Market Model/Daily_Stock_Report/StocksBTC-GBP.csv'\n",
    "    df2 = pd.read_csv(csv_path)\n",
    "    day_hashmap = {}\n",
    "    # 2022 filter\n",
    "    start_date = '2022-01-01'  \n",
    "    end_date = '2022-12-31'    \n",
    "    filtered_df = df2[(df2['Date'] >= start_date) & (df2['Date'] <= end_date)]\n",
    "    filtered_hashmap = {date: value for date, value in day_hashmap.items() if date.startswith('2022')}\n",
    "    date_sell_hashmap = {pd.to_datetime(key): value for key, value in filtered_hashmap.items()}\n",
    "    # Initial investment \n",
    "    initial_amount = 1000  \n",
    "    # Current investment amount\n",
    "    current_amount = initial_amount  \n",
    "    # Percentage of current amount to buy\n",
    "    buy_percentage = 1  \n",
    "    stock_prices = list(filtered_df['Close'])\n",
    "    buy_sell_metric = list(date_sell_hashmap.values())\n",
    "\n",
    "    # Iterate through each day\n",
    "    for i in range(1, len(stock_prices)):\n",
    "        percentage_change = (stock_prices[i] - stock_prices[i-1]) / stock_prices[i-1] * 100\n",
    "        current_amount *= (1 + percentage_change/100)\n",
    "        print(current_amount)\n",
    "        # Check buy/sell metric for the day\n",
    "        if buy_sell_metric[i] == 0:\n",
    "            # Calculate amount to buy\n",
    "            buy_amount = current_amount * (buy_percentage / 100)\n",
    "            # Update current amount\n",
    "            current_amount += buy_amount\n",
    "        elif buy_sell_metric[i] == 1:\n",
    "            # Dont do anything if sell\n",
    "            pass\n",
    "\n",
    "    # Calculate final amount\n",
    "    final_amount = current_amount\n",
    "    print(\"Final amount:\", final_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profitability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgBoost(X_train,y_train,X_test,y_test):\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    params = {\n",
    "    # For binary classification\n",
    "    'objective': 'binary:logistic',  \n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 100,\n",
    "    # Use log loss as the evaluation metric\n",
    "    'eval_metric': 'logloss'  \n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    model = xgb.train(params, dtrain, num_boost_round=10)\n",
    "\n",
    "    predictions = model.predict(dtest)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions.round())\n",
    "    print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for embedding_method in embeddings:\n",
    "    \n",
    "    # Run all embeddings on XgBoost\n",
    "\n",
    "    if embedding_method == 'TF-IDF':\n",
    "        X_embedding = X_tfidf\n",
    "    elif embedding_method == 'GloVe':\n",
    "        X_embedding = np.array([text_to_glove_embedding(text) for text in corpus])\n",
    "    elif embedding_method == 'UTF':\n",
    "        X_embedding = np.array([text_to_use_embedding(text) for text in corpus])\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_embedding, df['sell'], test_size=0.33, random_state=42)\n",
    "\n",
    "    xgBoost(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_individual():\n",
    "    individual = []\n",
    "    individual.append(random.randint(0, 2))  # classifier\n",
    "    individual.append(random.randint(0, 2))  # embedding\n",
    "    individual.append(random.randint(0, 2))  # param1\n",
    "    individual.append(random.randint(0, 2))  # param2\n",
    "    individual.append(random.randint(0, 2))  # alpha\n",
    "    return individual\n",
    "\n",
    "# Set up the Genetic Algorithm\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"individual\", tools.initIterate, creator.Individual, create_individual)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"evaluate\", eval)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=1, indpb=0.2)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Set up the population and evolve\n",
    "    hof = tools.HallOfFame(maxsize=5)\n",
    "    population = toolbox.population(n=10)\n",
    "    algorithms.eaMuPlusLambda(population, toolbox, mu=10, lambda_=20, cxpb=0.7, mutpb=0.2, ngen=10, stats=None, halloffame=None, verbose=True)\n",
    "\n",
    "    # Get the best individual from the final population\n",
    "    best_individual = tools.selBest(population, k=1)[0]\n",
    "    print(\"Best Individual:\", best_individual)\n",
    "    print(\"Best Accuracy:\", 1 / best_individual.fitness.values[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
